The low, near constant mean time for a byte read seen leading up to buffer size 64 kilobytes (kb) demonstrates the quick retrieval time for data from the L1 cache, because these low buffer sizes fit inside the cache. From this data we could conjecture that the cache is around 32 kb. 

As the buffer size escalates from 32 kb to 2048 kb, we observe three 'steps' of read time growth at 64, 256, 1024 kb respectively, which we believe correspond to increasing buffer sizes' increasing inabilities to fit into each slightly larger high level cache-- first the L2 cache is accessed, then the L3, and according to our step criteria, it seems like there is an L4, although we aren't sure if the data could be interpreted differently to instead indicate just L1, L2 and L3 cache steps.

Between 2048 and 4096 kb, the graph begins to curve upward sharply, and then after 4096 kb it spikes upward to its maximum level of ~100 ns per memory read.  We believe that this is because our buffer is being retrieved from both the L2 and L3 caches between 2048 and 4096 kb, and is being retrieved mostly from main memory after 4096 kb because of its prohibitive size.  

Because of our array population method for fooling the prefetcher, once a single memory item is read, all other memory items must be read before it can be read again. This should make it very difficult for caches to optimize performance without being able to contain our entire buffer size. We think this could explain why such a sharp spike in read time can be seen at 2048 KB, where it is impossible for the whole buffer to fit in any of the caches and many reads are being done. 

Our program is pretty slow. We attribute this to our array population method, which was our best idea for fooling the prefetcher with a complete cycle of indices for memory reads. We think that population process demands O(n^2) time, which adds up as the buffer sizes become quite large. Our earlier array algorithms that simply used random numbers were much faster, but they failed to give accurate data for very large byte sizes-- with purely random indices, we think we were sometimes reading in short cycles that could easily be detected by the prefetcher. 

Our numbers match up pretty closely both with the numbers provided in the assignment (in terms of time to read bytes), and with the numbers we got for Betsy's laptop (our test computer) when we looked them up.  Using the 'sudo lscpu' instruction in the virtual machine terminal, we found that her computer had 32 KB of L1 cache, 256 KB of L2 cache, and 4096 KB of L3 cache.  